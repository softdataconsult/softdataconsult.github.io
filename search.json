[
  {
    "objectID": "trainings/index.html",
    "href": "trainings/index.html",
    "title": "Trainings",
    "section": "",
    "text": "As a consultant and trainer, I leverage cutting-edge tools like R and Python to guide clients in extracting maximum value from their data assets. My services encompass strategic consultancy, hands-on training, and mentorship programs tailored to specific needs. From optimizing operations to uncovering growth opportunities, I provide comprehensive solutions for data-driven success."
  },
  {
    "objectID": "trainings/index.html#learning-python",
    "href": "trainings/index.html#learning-python",
    "title": "Trainings",
    "section": "Learning Python",
    "text": "Learning Python\nData analytics using Python offers a robust framework for extracting valuable insights from data. By mastering some foundational concepts and tools, students can build a solid foundation in data analytics and prepare for advanced topics in machine learning and data science.\n\nMy Training Portal"
  },
  {
    "objectID": "trainings/index.html#learning-r",
    "href": "trainings/index.html#learning-r",
    "title": "Trainings",
    "section": "Learning R",
    "text": "Learning R\nThis course provides an introduction to R programming for undergraduate students, focusing on fundamental concepts and practical applications in data analysis. Students will learn how to write R scripts, manage data, and create visualizations using R’s versatile tools. The course is designed to build a solid foundation in R programming, enabling students to tackle a wide range of data analysis tasks in their academic and professional pursuits.\n\nMy Training Portal\n\nView the SoftData Consult Learning Portal)"
  },
  {
    "objectID": "data_analytics/index.html",
    "href": "data_analytics/index.html",
    "title": "Data Analytics and Training",
    "section": "",
    "text": "As a Senior Consultant, I have honed my expertise in R Markdown, Python, Stata, SmartPLS, NVivo, etc, equipping organizations with the tools to decipher intricate data landscapes. My approach is rooted in a mission to empower through knowledge, ensuring that every statistical strategy is tailored to promote growth and innovation.\n\nData Analytics Using Python\n\nMy Training Portal\nData analytics using Python offers a robust framework for extracting valuable insights from data. By mastering some foundational concepts and tools, students can build a solid foundation in data analytics and prepare for advanced topics in machine learning and data science.\n\n\nData Analytics Using R\n\nMy Training Portal\nThis course provides an introduction to R programming for undergraduate students, focusing on fundamental concepts and practical applications in data analysis. Students will learn how to write R scripts, manage data, and create visualizations using R’s versatile tools. The course is designed to build a solid foundation in R programming, enabling students to tackle a wide range of data analysis tasks in their academic and professional pursuits.\n\n\nData Analytics Using Stata\n\nMy Training Portal\nEconometrics analysis using Stata equips students with the tools and techniques necessary to perform sophisticated data analysis in economics. By mastering Stata, students can effectively analyze economic data, test hypotheses, and contribute to informed policy-making\n\n\nSPSS for Data Analytics\n\nMy Training Portal\nWelcome to our comprehensive SPSS training course, designed to transform beginners into proficient data analysts in just eight weeks! As we delve into the world of statistical analysis using SPSS, you’ll unlock the power of data-driven decision-making, mastering techniques from basic descriptive statistics to advanced inferential analysis. Whether you’re looking to enhance your research capabilities, boost your career prospects, or simply gain a deeper understanding of data, this course offers a practical, hands-on approach that will equip you with the skills to tackle real-world data challenges confidently. Get ready to embark on an exciting journey into the realm of statistics and discover how to turn raw data into meaningful insights with SPSS!\n\nView the SoftData Consult Learning Portal"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/posts/R/index.html",
    "href": "blog/posts/R/index.html",
    "title": "Data Analytics & Visualization Using R",
    "section": "",
    "text": "Master the art of data analysis with R! In this course, you’ll learn how to explore, visualize, and analyze data using R, a powerful tool for statistical computing."
  },
  {
    "objectID": "blog/posts/spatial/index.html",
    "href": "blog/posts/spatial/index.html",
    "title": "Spatial Econometrics Data Analysis",
    "section": "",
    "text": "In this blog, I will demonstrate how I carried out Spatial Analysis of Reading Culture Among Higher Education Students in Southwestern Nigeria.\n\nIntroduction\n\n\nLoad the necessary libraries\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(splm)\nlibrary(spatialreg)\nlibrary(sp)\nlibrary(stargazer)\n\n\n\nReading shape file containing the data\n\n## Reading shape file containing the data\nreading = st_read(\"reading_cultre.shp\",quiet = TRUE)\nnames(reading) #show variable names\n\n#&gt;  [1] \"ID_0\"       \"ISO\"        \"NAME_0\"     \"ID_1\"       \"NAME_1\"    \n#&gt;  [6] \"TYPE_1\"     \"ENGTYPE_1\"  \"NL_NAME_1\"  \"VARNAME_1\"  \"reading_hr\"\n#&gt; [11] \"books_read\" \"cgpa\"       \"love_readi\" \"met_standa\" \"finis_book\"\n#&gt; [16] \"long\"       \"lat\"        \"geometry\"\n\nsummary(reading)\n\n#&gt;       ID_0         ISO               NAME_0               ID_1      \n#&gt;  Min.   :163   Length:38          Length:38          Min.   : 1.00  \n#&gt;  1st Qu.:163   Class :character   Class :character   1st Qu.:10.25  \n#&gt;  Median :163   Mode  :character   Mode  :character   Median :19.50  \n#&gt;  Mean   :163                                         Mean   :19.50  \n#&gt;  3rd Qu.:163                                         3rd Qu.:28.75  \n#&gt;  Max.   :163                                         Max.   :38.00  \n#&gt;     NAME_1             TYPE_1           ENGTYPE_1          NL_NAME_1        \n#&gt;  Length:38          Length:38          Length:38          Length:38         \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;   VARNAME_1           reading_hr      books_read          cgpa      \n#&gt;  Length:38          Min.   :0.000   Min.   : 0.000   Min.   :0.000  \n#&gt;  Class :character   1st Qu.:2.250   1st Qu.: 3.000   1st Qu.:2.962  \n#&gt;  Mode  :character   Median :2.735   Median : 4.000   Median :3.085  \n#&gt;                     Mean   :2.753   Mean   : 4.395   Mean   :2.904  \n#&gt;                     3rd Qu.:3.060   3rd Qu.: 5.000   3rd Qu.:3.413  \n#&gt;                     Max.   :6.000   Max.   :12.000   Max.   :3.930  \n#&gt;    love_readi      met_standa      finis_book         long       \n#&gt;  Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   : 3.474  \n#&gt;  1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.: 5.672  \n#&gt;  Median :2.000   Median :2.000   Median :2.000   Median : 7.314  \n#&gt;  Mean   :1.974   Mean   :1.947   Mean   :1.895   Mean   : 7.556  \n#&gt;  3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 8.705  \n#&gt;  Max.   :3.000   Max.   :3.000   Max.   :3.000   Max.   :14.450  \n#&gt;       lat                  geometry \n#&gt;  Min.   : 4.772   MULTIPOLYGON :38  \n#&gt;  1st Qu.: 6.525   epsg:4326    : 0  \n#&gt;  Median : 8.089   +proj=long...: 0  \n#&gt;  Mean   : 8.597                     \n#&gt;  3rd Qu.:10.686                     \n#&gt;  Max.   :13.110\n\nhead(reading)\n\n\n  \n\n\nplot(reading)\n\n\n\n\n\n\n\n\n\nclass(reading)\n\n#&gt; [1] \"sf\"         \"data.frame\"\n\nstr(reading)\n\n#&gt; Classes 'sf' and 'data.frame':   38 obs. of  18 variables:\n#&gt;  $ ID_0      : num  163 163 163 163 163 163 163 163 163 163 ...\n#&gt;  $ ISO       : chr  \"NGA\" \"NGA\" \"NGA\" \"NGA\" ...\n#&gt;  $ NAME_0    : chr  \"Nigeria\" \"Nigeria\" \"Nigeria\" \"Nigeria\" ...\n#&gt;  $ ID_1      : num  1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ NAME_1    : chr  \"Abia\" \"Adamawa\" \"Akwa Ibom\" \"Anambra\" ...\n#&gt;  $ TYPE_1    : chr  \"State\" \"State\" \"State\" \"State\" ...\n#&gt;  $ ENGTYPE_1 : chr  \"State\" \"State\" \"State\" \"State\" ...\n#&gt;  $ NL_NAME_1 : chr  NA NA NA NA ...\n#&gt;  $ VARNAME_1 : chr  NA NA NA NA ...\n#&gt;  $ reading_hr: num  3.06 2.06 2.2 2.66 2.75 2.25 3.06 2 3.5 2.42 ...\n#&gt;  $ books_read: num  4 5 3 4 4 5 5 2 3 3 ...\n#&gt;  $ cgpa      : num  3.01 2.69 3.52 3.43 3.13 2.76 3.13 2 3.51 3.5 ...\n#&gt;  $ love_readi: num  2 3 2 2 2 3 2 1 2 2 ...\n#&gt;  $ met_standa: num  2 2 2 2 3 2 2 2 2 2 ...\n#&gt;  $ finis_book: num  2 2 2 2 2 2 2 1 2 2 ...\n#&gt;  $ long      : num  7.52 12.4 7.85 6.94 9.99 ...\n#&gt;  $ lat       : num  5.46 9.32 4.91 6.23 10.78 ...\n#&gt;  $ geometry  :sfc_MULTIPOLYGON of length 38; first list element: List of 1\n#&gt;   ..$ :List of 1\n#&gt;   .. ..$ : num [1:302, 1:2] 7.51 7.52 7.53 7.53 7.53 ...\n#&gt;   ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n#&gt;  - attr(*, \"sf_column\")= chr \"geometry\"\n#&gt;  - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n#&gt;   ..- attr(*, \"names\")= chr [1:17] \"ID_0\" \"ISO\" \"NAME_0\" \"ID_1\" ...\n\nst_is_longlat(reading) # checking whether the geographical coordinates have been projected (the result TRUE means not) \n\n#&gt; [1] TRUE\n\nst_crs(reading) #checking which mapping was applied\n\n#&gt; Coordinate Reference System:\n#&gt;   User input: WGS 84 \n#&gt;   wkt:\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     DATUM[\"World Geodetic System 1984\",\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"latitude\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"longitude\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     ID[\"EPSG\",4326]]\n\ntable(st_is_valid(reading)) # validation\n\n#&gt; \n#&gt; TRUE \n#&gt;   38\n\nreading_sp&lt;-as(reading, \"Spatial\") \nclass(reading_sp)\n\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\n\n\n\nreading_points&lt;-st_cast(reading$geometry, \"MULTIPOINT\")\n\n\nreading_points_count&lt;-sapply(reading_points, length)\nsum(reading_points_count) # Checking how many vertices are in all counties\n\n#&gt; [1] 401800\n\n\n\nreading_simple&lt;-st_simplify(reading, dTolerance = 50)\n\n\nreading_simple_points&lt;-st_cast(reading_simple$geometry, \"MULTIPOINT\")\nsum(sapply(reading_simple_points, length))\n\n#&gt; [1] 73496\n\n\n\nreading_central&lt;-st_centroid(reading)\n\n\nplot(st_geometry(reading))\nreading_central&lt;-st_centroid(reading)\nplot(reading_central$geometry, add=TRUE, pch=20, col=\"red\")\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(reading_simple) + geom_sf() + theme_bw() \n\n\n\n\n\n\n\nggplot(reading_central) + geom_sf() + theme_bw() \n\n\n\n\n\n\n\n\n\nggplot(reading_simple) + geom_sf(aes(fill = reading_hr)) + theme_bw()\n\n\n\n\n\n\n\n\n\nggplot() + geom_sf(data=reading_simple, aes(fill=reading_hr)) +\ngeom_sf(data=reading_central, col=\"red\") + theme_bw()\n\n\n\n\n\n\n\n\n\nggplot() + geom_sf(data=reading_simple, aes(fill=books_read)) +\ngeom_sf(data=reading_central, col=\"red\") + theme_bw()\n\n\n\n\n\n\n\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf)\n\n\n\n\n\n\n\n\n\nnames(reading)\n\n#&gt;  [1] \"ID_0\"       \"ISO\"        \"NAME_0\"     \"ID_1\"       \"NAME_1\"    \n#&gt;  [6] \"TYPE_1\"     \"ENGTYPE_1\"  \"NL_NAME_1\"  \"VARNAME_1\"  \"reading_hr\"\n#&gt; [11] \"books_read\" \"cgpa\"       \"love_readi\" \"met_standa\" \"finis_book\"\n#&gt; [16] \"long\"       \"lat\"        \"geometry\"\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"reading_hr\"], \n     main = \"Spatial distn. of students' reading hour\", \n     breaks = \"quantile\")\n\n\n\n\n\n\n\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"books_read\"], \n     main = \"Spatial distn. of number of books read\", \n     breaks = \"quantile\")\n\n\n\n\n\n\n\n#legend(\"topright\", legend = \"books_read\", fill = topo.colors(5))\n\n\nreading_sf &lt;- st_read(\"reading_cultre.shp\")\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"cgpa\"], main = \"Spatial Distribution of Students' CGPA\", breaks = \"quantile\")\n\n\n\n\n\n\n\n# Add a legend\n#legend(\"topright\", legend = \"CGPA\", fill = topo.colors(5))\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"love_readi\"], \n     main = \"Spatial distn. of students who love reading\", \n     breaks = \"quantile\")\n\n\n\n\n\n\n\n#legend(\"topright\", legend = \"Love reading\", fill = topo.colors(5))\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"met_standa\"], \n     main = \"Spatial distn. of readers who read at least 1 hour per day\", \n     breaks = \"quantile\")\n\n\n\n\n\n\n\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"finis_book\"], \n     main = \"Spatial distn. of readers who finished their last book\", \n     breaks = \"quantile\")\n\n\n\n\n\n\n\n\n\n\nSix maps in one frame\n\nlibrary(tmap)\n\n# Read shapefile\nreading_sf &lt;- st_read(\"reading_cultre.shp\")\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\n# Set up a 2x3 layout with tmap\ntm_layout &lt;- tm_layout(title = c(\"Reading Hour\", \"Books Read\", \"CGPA\", \"Love Reading\", \"At least 1 hr/day\", \"Finished last book\"),\n                       frame = FALSE,\n                       asp = 0)  # Set aspect ratio to 0 for individual map customization\n\n# Plot the first map\ntm1 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"reading_hr\", title = \"Reading Hour\", style = \"quantile\")\n\n# Plot the second map\ntm2 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"books_read\", title = \"Number of Books Read\", style = \"quantile\")\n\n# Plot the third map\ntm3 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"cgpa\", title = \"CGPA\", style = \"quantile\")\n\n# Plot the fourth map\ntm4 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"love_readi\", title = \"Love Reading\", style = \"quantile\")\n\n# Plot the fifth map\ntm5 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"met_standa\", title = \"At least 1 hr/day\", style = \"quantile\")\n\n# Plot the sixth map\ntm6 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"finis_book\", title = \"Finished last book\", style = \"quantile\")\n\n# Display the maps in a 2x3 layout\ntmap_arrange(list(tm1, tm2, tm3, tm4, tm5, tm6), layout = tm_layout)\n\n\n\n\n\n\n\n\n\n# read the data from excel for other analysis\nreading_data &lt;- read.csv(\"reading_culture_coordinates.csv\")\n\n\n#define our regression equation so we don't have to type it each time\nreg.eq1=reading_hr ~ books_read + cgpa + love_readi + met_standa + finis_book\nreg1=lm(reg.eq1,data=reading)\n\n\n\nSpatial weights matrix W\n\n# Create a spatial weights matrix (queen contiguity)\nlistw &lt;- mat2listw(matrix(rbinom(nrow(reading_data)^2, 1, 0.2), nrow(reading_data)))\n\nlistw\n\n#&gt; Characteristics of weights list object:\n#&gt; Neighbour list object:\n#&gt; Number of regions: 38 \n#&gt; Number of nonzero links: 293 \n#&gt; Percentage nonzero weights: 20.29086 \n#&gt; Average number of links: 7.710526 \n#&gt; Non-symmetric neighbours list\n#&gt; \n#&gt; Weights style: M \n#&gt; Weights constants summary:\n#&gt;    n   nn  S0  S1   S2\n#&gt; M 38 1444 293 353 9516\n\n\n\n\nlm.morantest\n\nlm.morantest(reg1, listw )\n\n#&gt; \n#&gt;  Global Moran I for regression residuals\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; weights: listw\n#&gt; \n#&gt; Moran I statistic standard deviate = 0.74448, p-value = 0.2283\n#&gt; alternative hypothesis: greater\n#&gt; sample estimates:\n#&gt; Observed Moran I      Expectation         Variance \n#&gt;      0.006178629     -0.031412369      0.002549551\n\n\nLet’s run the Four simplest models: OLS, SLX, Lag Y, and Lag Error\nOLS model\n\nlibrary(spdep)\nlibrary(spatialreg)\nreg1=lm(reg.eq1,data=reading)\nreg1b=lm(reading_hr ~ books_read + cgpa + love_readi + met_standa + finis_book, data=reading)\nsummary(reg1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = reg.eq1, data = reading)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.5992 -0.5211 -0.1218  0.1665  3.3228 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept)  0.07031    0.51720   0.136   0.8927  \n#&gt; books_read   0.07339    0.08281   0.886   0.3821  \n#&gt; cgpa         0.76432    0.33255   2.298   0.0282 *\n#&gt; love_readi   0.30082    0.49804   0.604   0.5501  \n#&gt; met_standa   0.53481    0.57056   0.937   0.3556  \n#&gt; finis_book  -0.78873    0.61250  -1.288   0.2071  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9277 on 32 degrees of freedom\n#&gt; Multiple R-squared:  0.4904, Adjusted R-squared:  0.4108 \n#&gt; F-statistic: 6.159 on 5 and 32 DF,  p-value: 0.000419\n\nsummary(reg1b)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = reading_hr ~ books_read + cgpa + love_readi + met_standa + \n#&gt;     finis_book, data = reading)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.5992 -0.5211 -0.1218  0.1665  3.3228 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept)  0.07031    0.51720   0.136   0.8927  \n#&gt; books_read   0.07339    0.08281   0.886   0.3821  \n#&gt; cgpa         0.76432    0.33255   2.298   0.0282 *\n#&gt; love_readi   0.30082    0.49804   0.604   0.5501  \n#&gt; met_standa   0.53481    0.57056   0.937   0.3556  \n#&gt; finis_book  -0.78873    0.61250  -1.288   0.2071  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9277 on 32 degrees of freedom\n#&gt; Multiple R-squared:  0.4904, Adjusted R-squared:  0.4108 \n#&gt; F-statistic: 6.159 on 5 and 32 DF,  p-value: 0.000419\n\nlm.morantest(reg1,listw)\n\n#&gt; \n#&gt;  Global Moran I for regression residuals\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; weights: listw\n#&gt; \n#&gt; Moran I statistic standard deviate = 0.74448, p-value = 0.2283\n#&gt; alternative hypothesis: greater\n#&gt; sample estimates:\n#&gt; Observed Moran I      Expectation         Variance \n#&gt;      0.006178629     -0.031412369      0.002549551\n\n#lm.moranplot(reg1,listw1)\nlm.LMtests(reg1,listw,test=c(\"LMerr\", \"LMlag\", \"RLMerr\", \"RLMlag\", \"SARMA\"))\n\n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; RSerr = 0.0092842, df = 1, p-value = 0.9232\n#&gt; \n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; RSlag = 1.4528, df = 1, p-value = 0.2281\n#&gt; \n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; adjRSerr = 0.11628, df = 1, p-value = 0.7331\n#&gt; \n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; adjRSlag = 1.5598, df = 1, p-value = 0.2117\n#&gt; \n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; SARMA = 1.5691, df = 2, p-value = 0.4563\n\n\nSLX (Spatial Lag Model with Exogenous Variables)\n\n# Assuming your data is named 'your_data'\n# Load required packages\nlibrary(spatialreg)\n\n# Create an 'sf' object with the spatial coordinates\nreading_sf_data &lt;- st_as_sf(reading_data, coords = c(\"long\", \"lat\"))\n\n# Create a spatial weights matrix (queen contiguity)\nlistw &lt;- mat2listw(matrix(rbinom(nrow(reading_data)^2, 1, 0.2), nrow(reading_data)))"
  },
  {
    "objectID": "blog/posts/python/index.html",
    "href": "blog/posts/python/index.html",
    "title": "One-way ANOVA using Python",
    "section": "",
    "text": "In this blog, we’ll explore how to perform a One-Way ANOVA in Python to compare the means of multiple groups and assess if they are statistically different."
  },
  {
    "objectID": "blog/posts/python/index.html#resources",
    "href": "blog/posts/python/index.html#resources",
    "title": "One-way ANOVA using Python",
    "section": "Resources",
    "text": "Resources\nYou can also watch One-way Anova using Python on my YouTube channel for details:"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog, where data meets discovery! Here, I train and guide aspiring data enthusiasts to unlock the power of tools like R, Python, STATA, and more. Whether you’re just starting or looking to deepen your skills, you’ll find practical insights and tutorials to help you master data analytics and turn information into actionable insights. Let’s embark on this journey of learning, growth, and innovation together!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Econometrics Data Analysis\n\n\n\n\n\n\nSpatial\n\n\nR\n\n\n\nLearn how to handle spatial data analysis using R. \n\n\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPartial Least Squares Structural Equation Modeling (PLS-SEM)\n\n\n\n\n\n\nPython\n\n\nR\n\n\nAnalytics\n\n\nSmartPLS\n\n\n\nThis course introduces you to Partial Least Squares Structural Equation Modeling (PLS-SEM), a statistical technique used to analyze complex relationships between observed and latent variables for predictive modeling and theory testing. \n\n\n\n\n\nOct 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOne-way ANOVA using Python\n\n\n\n\n\n\nPython\n\n\nR\n\n\nAnalytics\n\n\n\nLearn how to carry out one-way analysis of variance using Python. \n\n\n\n\n\nOct 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTime series analysis on Website Traffic Data\n\n\n\n\n\n\nPython\n\n\n\nLearn how to apply time series analysis methods on website traffic datasets. \n\n\n\n\n\nOct 7, 2024\n\n\nIsaac Ajao\n\n\n\n\n\n\n\n\n\n\n\n\nData Analytics Using Stata\n\n\n\n\n\n\nPython\n\n\nR\n\n\nStata\n\n\nspss\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData Analytics Using SPSS\n\n\n\n\n\n\nPython\n\n\nR\n\n\nStata\n\n\nspss\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData Analytics & Visualization Using R\n\n\n\n\n\n\nPython\n\n\nR\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "home/index.html",
    "href": "home/index.html",
    "title": "Home",
    "section": "",
    "text": "As a consultant and trainer, I leverage cutting-edge tools like R and Python to guide clients in extracting maximum value from their data assets. My services encompass strategic consultancy, hands-on training, and mentorship programs tailored to specific needs. From optimizing operations to uncovering growth opportunities, I provide comprehensive solutions for data-driven success."
  },
  {
    "objectID": "home/index.html#training-on-statistical-data-analytics",
    "href": "home/index.html#training-on-statistical-data-analytics",
    "title": "Home",
    "section": "",
    "text": "As a consultant and trainer, I leverage cutting-edge tools like R and Python to guide clients in extracting maximum value from their data assets. My services encompass strategic consultancy, hands-on training, and mentorship programs tailored to specific needs. From optimizing operations to uncovering growth opportunities, I provide comprehensive solutions for data-driven success."
  },
  {
    "objectID": "blog/posts/pls-sem/index.html",
    "href": "blog/posts/pls-sem/index.html",
    "title": "Partial Least Squares Structural Equation Modeling (PLS-SEM)",
    "section": "",
    "text": "Introduction\nPartial Least Squares Structural Equation Modeling (PLS-SEM) is a statistical approach used to estimate complex cause-effect relationship models involving latent constructs and their indicators. It’s widely used in fields like marketing, management, psychology, and social sciences, particularly when traditional covariance-based SEM (CB-SEM) assumptions are too restrictive or the data do not meet normality or sample size requirements.\n\n\nKey Concepts in PLS-SEM\n\nLatent Variables (LVs): Latent variables are unobserved variables that are inferred from multiple observed indicators. In PLS-SEM, these are often theoretical constructs like “Customer Satisfaction” or “Brand Loyalty.” LVs are measured indirectly via observed variables, called indicators.\nIndicators: These are the observed variables or items (such as survey questions or measurable attributes) used to measure the latent variables. For instance, “Customer Satisfaction” could be measured using a set of questions where each question represents an indicator.\nMeasurement Model (Outer Model): This part of the model defines the relationship between the latent variables and their respective indicators. There are two types of measurement models:\n\nReflective Measurement Model: Indicators are caused by the latent variable (e.g., “Customer Satisfaction” causes the survey responses).\nFormative Measurement Model: Indicators form or define the latent variable (e.g., “Income” and “Education” collectively form “Socioeconomic Status”).\n\nStructural Model (Inner Model): This represents the relationships between latent variables, defining the hypothesized paths and cause-effect relationships in the model. For example, “Customer Satisfaction” might influence “Customer Loyalty.”\nPath Coefficients: These are the estimates of the relationships between latent variables in the structural model. They indicate the strength and direction of the relationships (similar to regression coefficients).\nWeights and Loadings:\n\nWeights are used to calculate latent variable scores from their indicators (especially in formative models).\nLoadings represent the strength of the relationship between indicators and latent variables in reflective models.\n\nR-squared: This is the measure of explained variance for the endogenous latent variables (dependent variables) in the model. It indicates how much variance in the latent variable is explained by the independent variables.\n\n\n\n\nWhen to Use PLS-SEM\nPLS-SEM is used when:\n\nThe model is complex with many latent variables and indicators.\nThe data do not meet the assumptions of normality or large sample size.\nThe goal is prediction or theory development (as opposed to theory confirmation).\nFormative constructs (where indicators form a latent variable) are present.\nThe model includes relationships that might not converge in CB-SEM.\n\nPLS-SEM is particularly useful when dealing with smaller samples, non-normal data, and exploratory research scenarios.\n\n\nAdvantages of PLS-SEM\n\nFlexibility: PLS-SEM can handle both reflective and formative constructs, and it doesn’t require strict distributional assumptions (i.e., no normality assumption).\nSmall Sample Size: PLS-SEM performs well with small to medium sample sizes, unlike CB-SEM, which requires larger samples.\nComplex Models: It is suited for models with a large number of constructs and relationships (even when these models may not converge using CB-SEM).\nFocus on Prediction: PLS-SEM is more concerned with maximizing the explained variance (prediction) of dependent variables rather than testing the fit of the entire model.\nBootstrapping for Significance: PLS-SEM uses bootstrapping techniques to generate standard errors and confidence intervals, allowing for hypothesis testing without relying on traditional parametric assumptions.\n\n\n\nSteps in PLS-SEM\n\nModel Specification: Specify the measurement model (linking latent variables and indicators) and the structural model (linking latent variables to each other).\nModel Estimation: PLS-SEM iteratively estimates relationships by maximizing the variance explained in the dependent latent variables. It first estimates the latent variable scores, then calculates the path coefficients.\nModel Evaluation: Evaluate the measurement model (using indicator reliability, internal consistency, and convergent validity) and the structural model (using path coefficients, R-squared, and predictive relevance).\nBootstrapping: This is used to estimate the precision of the PLS estimates and to assess the significance of the path coefficients (p-values) in the structural model.\n\n\n\nModel Evaluation Criteria\n\nMeasurement Model:\n\nReliability: Evaluated using indicator loadings and composite reliability.\nValidity: Checked with average variance extracted (AVE), discriminant validity (HTMT or Fornell-Larcker criterion).\nCollinearity: VIF (Variance Inflation Factor) to ensure no multicollinearity in formative constructs.\n\nStructural Model:\n\nPath Coefficients: The strength and significance of relationships between latent variables.\nR-squared (R²): Indicates the variance explained by independent latent variables for each endogenous latent variable.\nPredictive Relevance (Q²): Assessed using blindfolding procedures to determine how well the model can predict.\n\nGoodness of Fit: Although traditional SEM focuses on model fit indices (like chi-square, RMSEA, etc.), PLS-SEM focuses more on the variance explained (R²) and predictive power rather than overall model fit.\n\n\n\nCommon Software for PLS-SEM\n\nSmartPLS: A popular commercial software with a graphical user interface.\nADANCO: Another commercial software specifically designed for PLS-SEM.\nR: Free and flexible tools like plspm, semPLS, and seminr.\nWarpPLS: A commercial tool that offers unique algorithms for PLS-SEM.\n\n\n\nPLS-SEM vs. Covariance-Based SEM (CB-SEM)\n\n\n\n\n\n\n\n\nFeature\nPLS-SEM\nCB-SEM (e.g., LISREL, AMOS)\n\n\n\n\nGoal\nPrediction, theory development\nTheory confirmation\n\n\nSample Size\nSmall to medium\nLarge\n\n\nAssumptions\nNon-parametric, no normality required\nParametric, requires normality\n\n\nModel Fit\nFocuses on R² and predictive power\nFit indices (e.g., RMSEA, CFI, etc.)\n\n\nHandling Complexity\nHandles complex models easily\nMay struggle with large, complex models\n\n\n\n\n\nApplications of PLS-SEM\n\nMarketing: Used for customer satisfaction and loyalty models.\nSocial Sciences: To explore relationships between behavioral constructs.\nBusiness: For evaluating business performance and strategy.\nInformation Systems: For studying technology acceptance models (e.g., TAM).\n\n\n\nConclusion\nPLS-SEM is a powerful and flexible method used to estimate complex models with latent variables and their indicators. It is especially suited for exploratory research and predictive modeling, offering an alternative to traditional covariance-based SEM when data and model conditions are not ideal.\nIf you found this article interesting and informative. The GitHub repository is here. You can follow me on Twitter and Linkedin for more updates on R, Python, STATA and Excel for data science."
  },
  {
    "objectID": "blog/posts/stata/index.html",
    "href": "blog/posts/stata/index.html",
    "title": "Data Analytics Using Stata",
    "section": "",
    "text": "Stata is a versatile software used for data analysis, statistical modeling, and visualization, popular among researchers and professionals. It offers powerful tools for handling complex datasets, making it ideal for econometrics, social science research, and more."
  },
  {
    "objectID": "blog/posts/spss/index.html",
    "href": "blog/posts/spss/index.html",
    "title": "Data Analytics Using SPSS",
    "section": "",
    "text": "SPSS is a powerful statistical software used for data analysis, management, and visualization in various fields. It simplifies complex statistical processes, making it easier to analyze data and generate meaningful insights."
  },
  {
    "objectID": "blog/posts/website-traffic/index.html",
    "href": "blog/posts/website-traffic/index.html",
    "title": "Time series analysis on Website Traffic Data",
    "section": "",
    "text": "Time series analysis provides a powerful framework for understanding and predicting website traffic patterns, enabling data-driven decision-making\n\nIntroduction\nIn today’s digital landscape, understanding and analyzing website traffic is crucial for businesses, bloggers, and developers. One powerful method for gaining insights from traffic data is time series analysis. Time series analysis helps to uncover patterns, trends, and seasonal behaviors in data that vary over time, making it an essential tool for optimizing website performance and decision-making.\nA time series is a sequence of data points collected at regular intervals over time. For websites, traffic data typically includes metrics like the number of daily visits, page views, or unique visitors. Time series analysis enables us to track these metrics and detect patterns, helping answer questions like:\n\nAre there predictable periods of high or low traffic?\nHow does traffic fluctuate during weekends or holidays?\nWhat long-term trends can be observed?\n\n\nKey Concepts in Time Series Analysis\n\nTrend: The overall direction the data is moving. For example, if the traffic to a website has been steadily increasing over several months, we might identify an upward trend.\nSeasonality: Recurring patterns that happen at regular intervals, such as daily or weekly traffic spikes. For instance, many websites experience increased traffic during weekdays and lower traffic on weekends.\nNoise: Random variations in the data that do not follow any specific pattern. Identifying and smoothing out noise helps highlight more significant trends and patterns.\nStationarity: A stationary time series has a constant mean and variance over time. Many time series methods require the data to be stationary, so transformation techniques like differencing or log transformations might be used to achieve stationarity.\n\n\n\nWhy Use Time Series Analysis for Website Traffic?\nWebsite traffic data often follows complex patterns due to factors like marketing campaigns, seasonal trends, and user behavior. Time series analysis allows us to:\n\nForecast future traffic: Using methods such as ARIMA (AutoRegressive Integrated Moving Average), you can predict how many visitors your site will have next month or next quarter.\nDetect anomalies: Sudden spikes or drops in traffic can indicate issues such as website downtime or a viral post.\nOptimize content and marketing strategies: By understanding traffic patterns, you can tailor your content release or marketing efforts to maximize impact during high-traffic periods.\n\n\n\nExample Use Case: Analyzing January Traffic for a Website\nLet’s say we want to analyze the daily website traffic for January 2024. We can collect the number of visits each day and visualize the data using a line chart. By applying time series analysis techniques, we can identify trends (e.g., an upward trend due to a new blog post), seasonality (e.g., reduced traffic on weekends), and detect any anomalies (e.g., traffic spikes after a major announcement).\nAfter analyzing the data, we could use forecasting models like ARIMA to predict the number of visits in February, allowing for better resource planning or marketing campaigns.\n\n\n\nSummary\nTime series analysis transforms website traffic data into actionable insights. Whether you’re aiming to forecast future traffic, optimize campaigns, or monitor site performance, understanding the patterns in your website traffic is essential for growth. With tools like Python, R, or even Excel, you can easily get started with time series analysis and take your website to the next level.\n\n\nLoad the necessary libraries\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Daily website traffic data for January 2024\ntraffic_data = [\n    113, 229, 262, 349, 268, 221, 177, 337, 340, 300, 302, 244, 151, 155, 320,\n    299, 330, 292, 298, 229, 191, 377, 384, 322, 314, 299, 184, 140, 230, 232, 194\n]\n\n\n# Create a pandas dataframe with the data\ndays = list(range(1, 32))  # Days of January\ndf = pd.DataFrame({\n    'Day': days,\n    'Traffic': traffic_data\n})\n\nprint(days)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n\n\n\n\nTime plot\n\n# Plot the daily website traffic\nplt.figure(figsize=(10, 6))\nplt.plot(df['Day'], df['Traffic'], marker='o', linestyle='-', color='b')\nplt.title('Daily Website Traffic for January 2024')\nplt.xlabel('Day of January')\nplt.ylabel('Website Traffic (Number of Visits)')\nplt.grid(True)\nplt.xticks(days)\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n# Display traffic statistics\ntraffic_trend = df['Traffic'].describe()\nprint(\"\\nDescriptive statistics\\n\", traffic_trend)\n\n\n\n\n\n\n\n\n\nDescriptive statistics\n count     31.000000\nmean     260.741935\nstd       72.933745\nmin      113.000000\n25%      207.500000\n50%      268.000000\n75%      317.000000\nmax      384.000000\nName: Traffic, dtype: float64\n\n\n\n\nUsing the Simple Moving Average (SMA)\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Example website traffic data for February 2024 (29 days in Feb)\ntraffic_data_feb = [\n    159,    185,    134,    115,    236,    214,    190,    186,    151,    140,    126,\n    188,    196,    133,    136,    106,    102,    69, 130,    161,    127,    28, 3,\n    1,  28, 115,    99, 66, 66\n\n]\n\n# Create a pandas dataframe with the data\ndays_feb = list(range(1, 30))  # Days of February (29 days)\ndf_feb = pd.DataFrame({\n    'Day': days_feb,\n    'Traffic': traffic_data_feb\n})\n\n# Calculate 3-day and 7-day simple moving averages\ndf_feb['3-day SMA'] = df_feb['Traffic'].rolling(window=3).mean()\ndf_feb['7-day SMA'] = df_feb['Traffic'].rolling(window=7).mean()\n\n# Plot original data, 3-day SMA, and 7-day SMA\nplt.figure(figsize=(10, 6))\n\n# Plot original data\nplt.plot(df_feb['Day'], df_feb['Traffic'], marker='o', linestyle='-', label='Original Data', color='b')\n\n# Plot 3-day SMA\nplt.plot(df_feb['Day'], df_feb['3-day SMA'], marker='o', linestyle='-', label='3-day SMA', color='g')\n\n# Plot 7-day SMA\nplt.plot(df_feb['Day'], df_feb['7-day SMA'], marker='o', linestyle='-', label='7-day SMA', color='r')\n\nplt.title('Website Traffic with 3-day and 7-day Simple Moving Averages (February 2024)')\nplt.xlabel('Day of February')\nplt.ylabel('Website Traffic (Number of Visits)')\nplt.grid(True)\nplt.legend()\nplt.xticks(days_feb)\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSeasonal Decomposition\n\nimport pandas as pd\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Create a pandas dataframe\ndata = pd.DataFrame({\n  \"date\": pd.date_range(start=\"2024-03-01\", end=\"2024-03-31\", freq=\"D\"),\n  \"traffic\": [99, 72, 86, 141, 95, 1, 1, 1, 0, 0, 42, 130, 188, 94, 68, \n              57, 67, 20, 3, 87, 49, 30, 82, 110, 162, 136, 155, 124, 102, 103, 84]\n})\n\n# Perform seasonal decomposition using an additive model\ndecomposition = seasonal_decompose(data[\"traffic\"], model=\"additive\", period=7)\n\n# Extract trend, seasonal, and residual components\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\n# Plot the components\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\n\nplt.subplot(311)\nplt.plot(data[\"date\"], trend, label=\"Trend\")\nplt.legend()\nplt.title(\"Trend\")\n\nplt.subplot(312)\nplt.plot(data[\"date\"], seasonal, label=\"Seasonal\")\nplt.legend()\nplt.title(\"Seasonal\")\n\nplt.subplot(313)\nplt.plot(data[\"date\"], residual, label=\"Residual\")\nplt.legend()\nplt.title(\"Residual\")\n\nplt.xlabel(\"Date\")\nplt.ylabel(\"Website Traffic\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Youtube\n  \n  \n    \n     Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\n\nWelcome to my website!\nI am a Data Analyst, Trainer, and Consultant specializing in Python, R, Stata, and more. With a passion for turning data into actionable insights, I provide expert consulting and training services tailored to individuals, businesses, and organizations. Whether you’re looking to master data analysis tools or need professional guidance on your data.\nAs a Senior Consultant, I have honed my expertise in R Markdown, SmartPLS, and NVivo, equipping organizations with the tools to decipher intricate data landscapes. My approach is rooted in a mission to empower through knowledge, ensuring that every statistical strategy is tailored to promote growth and innovation.\nStatistical Analysis & Training:\n\nConducted over 50 data analysis workshops for organizations across sectors\nConsulted on 25+ projects, helping clients gain competitive insights through data-driven strategies\nTrained 300+ professionals in advanced statistical methodologies\nCurrently training 2000 students virtually on R programming\n\nSoftware Development:\n\nDeveloped 5 user-friendly statistical software solutions, streamlining analysis for 100+ users\nImproved client productivity by 30% with efficient data visualization and reporting tools\n\nAcademic Editing:\n\nReviewed and edited 200+ scholarly articles, ensuring adherence to standards\nAssisted 75+ researchers in refining their work for publication in esteemed journals\n\nMultimedia Production:\n\nCreated compelling visual content for 30+ clients through professional photography/videography\nProduced 20+ multimedia marketing campaigns, boosting client brand visibility\n\nOnline Education:\n\nFacilitated 15+ online courses, providing accessible training to 500+ learners globally\nDeveloped interactive e-learning modules with 90% participant satisfaction rate\n\nOn this website, you will find a selection of my published works, including scholarly articles, monographs, and translations. You will also find news and updates on my current research projects, as well as information on upcoming events and conferences where I will be speaking. Please feel free to contact me if you have any questions or would like to discuss potential projects.\n\nView the SoftData Consult Learning Portal"
  }
]