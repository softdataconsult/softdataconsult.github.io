[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Youtube\n  \n  \n    \n     Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\n\nWelcome to my website!\nI am a Data Analyst, Trainer, and Consultant specializing in Python, R, Stata, and more. With a passion for turning data into actionable insights, I provide expert consulting and training services tailored to individuals, businesses, and organizations. Whether you’re looking to master data analysis tools or need professional guidance on your data.\nAs a Senior Consultant, I have honed my expertise in R Markdown, SmartPLS, and NVivo, equipping organizations with the tools to decipher intricate data landscapes. My approach is rooted in a mission to empower through knowledge, ensuring that every statistical strategy is tailored to promote growth and innovation.\nStatistical Analysis & Training:\n\nConducted over 50 data analysis workshops for organizations across sectors\nConsulted on 25+ projects, helping clients gain competitive insights through data-driven strategies\nTrained 300+ professionals in advanced statistical methodologies\nCurrently training 2000 students virtually on R programming\n\nSoftware Development:\n\nDeveloped 5 user-friendly statistical software solutions, streamlining analysis for 100+ users\nImproved client productivity by 30% with efficient data visualization and reporting tools\n\nAcademic Editing:\n\nReviewed and edited 200+ scholarly articles, ensuring adherence to standards\nAssisted 75+ researchers in refining their work for publication in esteemed journals\n\nMultimedia Production:\n\nCreated compelling visual content for 30+ clients through professional photography/videography\nProduced 20+ multimedia marketing campaigns, boosting client brand visibility\n\nOnline Education:\n\nFacilitated 15+ online courses, providing accessible training to 500+ learners globally\nDeveloped interactive e-learning modules with 90% participant satisfaction rate\n\nOn this website, you will find a selection of my published works, including scholarly articles, monographs, and translations. You will also find news and updates on my current research projects, as well as information on upcoming events and conferences where I will be speaking. Please feel free to contact me if you have any questions or would like to discuss potential projects.\n\nView the SoftData Consult Learning Portal"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog, where data meets discovery! Here, I train and guide aspiring data enthusiasts to unlock the power of tools like R, Python, STATA, and more. Whether you’re just starting or looking to deepen your skills, you’ll find practical insights and tutorials to help you master data analytics and turn information into actionable insights. Let’s embark on this journey of learning, growth, and innovation together!\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPartial Least Squares Structural Equation Modeling (PLS-SEM)\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nAnalytics\n\n\nSmartPLS\n\n\n\n\nThis course introduces you to Partial Least Squares Structural Equation Modeling (PLS-SEM), a statistical technique used to analyze complex relationships between observed and latent variables for predictive modeling and theory testing.\n\n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n  \n\n\n\n\nSpatial Econometrics Data Analysis\n\n\n\n\n\n\n\nSpatial\n\n\nR\n\n\n\n\nLearn how to handle spatial data analysis using R.\n\n\n\n\n\n\nOct 11, 2024\n\n\nIsaac Ajao\n\n\n\n\n\n\n  \n\n\n\n\nOne-way ANOVA using Python\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nAnalytics\n\n\n\n\nLearn how to carry out one-way analysis of variance using Python.\n\n\n\n\n\n\nOct 7, 2024\n\n\n\n\n\n\n  \n\n\n\n\nTime series analysis on Website Traffic Data\n\n\n\n\n\n\n\nPython\n\n\n\n\nLearn how to apply time series analysis methods on website traffic datasets.\n\n\n\n\n\n\nOct 7, 2024\n\n\nIsaac Ajao\n\n\n\n\n\n\n  \n\n\n\n\nData Analytics & Visualization Using R\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n\n\n\n\n  \n\n\n\n\nData Analytics Using SPSS\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nStata\n\n\nspss\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n\n\n\n\n  \n\n\n\n\nData Analytics Using Stata\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\nStata\n\n\nspss\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/spatial/index.html",
    "href": "blog/posts/spatial/index.html",
    "title": "Spatial Econometrics Data Analysis",
    "section": "",
    "text": "In this blog, I will demonstrate how to perform spatial data analysis using R, focusing on techniques for exploring and modeling spatial relationships in your data. You’ll learn how to handle spatial data, visualize spatial patterns, and apply spatial econometric models to gain deeper insights into spatial dependencies. Case study: Reading Culture Among Higher Education Students in Southwestern Nigeria.\nIntroduction\nSpatial econometrics is a branch of econometrics that deals with the modeling and analysis of spatially dependent data. In traditional econometrics, observations are often assumed to be independent of each other, but in spatial econometrics, it is recognized that observations from nearby locations can influence each other, creating spatial dependence.\nThis analysis incorporates spatial relationships into models through spatial weights matrices, which define the structure of these dependencies. Two common forms of spatial dependence are spatial lag (where the value of a variable in one location depends on the values of the same variable in nearby locations) and spatial error (where errors in one location are correlated with errors in nearby locations).\nSpatial econometrics is especially useful in fields like regional economics, real estate, environmental studies, and geography, where spatial factors significantly impact the relationships being studied. It allows for more accurate modeling and understanding of phenomena such as housing prices, land use, and even the spread of diseases, taking into account the spatial proximity of observations.\nLoad the necessary libraries\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(splm)\nlibrary(spatialreg)\nlibrary(sp)\nlibrary(stargazer)\n\nReading shape file containing the data\n\n## Reading shape file containing the data\nreading = st_read(\"reading_cultre.shp\",quiet = TRUE)\nnames(reading) #show variable names\n\n#&gt;  [1] \"ID_0\"       \"ISO\"        \"NAME_0\"     \"ID_1\"       \"NAME_1\"    \n#&gt;  [6] \"TYPE_1\"     \"ENGTYPE_1\"  \"NL_NAME_1\"  \"VARNAME_1\"  \"reading_hr\"\n#&gt; [11] \"books_read\" \"cgpa\"       \"love_readi\" \"met_standa\" \"finis_book\"\n#&gt; [16] \"long\"       \"lat\"        \"geometry\"\n\nsummary(reading)\n\n#&gt;       ID_0         ISO               NAME_0               ID_1      \n#&gt;  Min.   :163   Length:38          Length:38          Min.   : 1.00  \n#&gt;  1st Qu.:163   Class :character   Class :character   1st Qu.:10.25  \n#&gt;  Median :163   Mode  :character   Mode  :character   Median :19.50  \n#&gt;  Mean   :163                                         Mean   :19.50  \n#&gt;  3rd Qu.:163                                         3rd Qu.:28.75  \n#&gt;  Max.   :163                                         Max.   :38.00  \n#&gt;     NAME_1             TYPE_1           ENGTYPE_1          NL_NAME_1        \n#&gt;  Length:38          Length:38          Length:38          Length:38         \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;   VARNAME_1           reading_hr      books_read          cgpa      \n#&gt;  Length:38          Min.   :0.000   Min.   : 0.000   Min.   :0.000  \n#&gt;  Class :character   1st Qu.:2.250   1st Qu.: 3.000   1st Qu.:2.962  \n#&gt;  Mode  :character   Median :2.735   Median : 4.000   Median :3.085  \n#&gt;                     Mean   :2.753   Mean   : 4.395   Mean   :2.904  \n#&gt;                     3rd Qu.:3.060   3rd Qu.: 5.000   3rd Qu.:3.413  \n#&gt;                     Max.   :6.000   Max.   :12.000   Max.   :3.930  \n#&gt;    love_readi      met_standa      finis_book         long       \n#&gt;  Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   : 3.474  \n#&gt;  1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.: 5.672  \n#&gt;  Median :2.000   Median :2.000   Median :2.000   Median : 7.314  \n#&gt;  Mean   :1.974   Mean   :1.947   Mean   :1.895   Mean   : 7.556  \n#&gt;  3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.: 8.705  \n#&gt;  Max.   :3.000   Max.   :3.000   Max.   :3.000   Max.   :14.450  \n#&gt;       lat                  geometry \n#&gt;  Min.   : 4.772   MULTIPOLYGON :38  \n#&gt;  1st Qu.: 6.525   epsg:4326    : 0  \n#&gt;  Median : 8.089   +proj=long...: 0  \n#&gt;  Mean   : 8.597                     \n#&gt;  3rd Qu.:10.686                     \n#&gt;  Max.   :13.110\n\nhead(reading)\n\n\n\n  \n\n\nplot(reading)\n\n\n\n\n\nclass(reading)\n\n#&gt; [1] \"sf\"         \"data.frame\"\n\nstr(reading)\n\n#&gt; Classes 'sf' and 'data.frame':   38 obs. of  18 variables:\n#&gt;  $ ID_0      : num  163 163 163 163 163 163 163 163 163 163 ...\n#&gt;  $ ISO       : chr  \"NGA\" \"NGA\" \"NGA\" \"NGA\" ...\n#&gt;  $ NAME_0    : chr  \"Nigeria\" \"Nigeria\" \"Nigeria\" \"Nigeria\" ...\n#&gt;  $ ID_1      : num  1 2 3 4 5 6 7 8 9 10 ...\n#&gt;  $ NAME_1    : chr  \"Abia\" \"Adamawa\" \"Akwa Ibom\" \"Anambra\" ...\n#&gt;  $ TYPE_1    : chr  \"State\" \"State\" \"State\" \"State\" ...\n#&gt;  $ ENGTYPE_1 : chr  \"State\" \"State\" \"State\" \"State\" ...\n#&gt;  $ NL_NAME_1 : chr  NA NA NA NA ...\n#&gt;  $ VARNAME_1 : chr  NA NA NA NA ...\n#&gt;  $ reading_hr: num  3.06 2.06 2.2 2.66 2.75 2.25 3.06 2 3.5 2.42 ...\n#&gt;  $ books_read: num  4 5 3 4 4 5 5 2 3 3 ...\n#&gt;  $ cgpa      : num  3.01 2.69 3.52 3.43 3.13 2.76 3.13 2 3.51 3.5 ...\n#&gt;  $ love_readi: num  2 3 2 2 2 3 2 1 2 2 ...\n#&gt;  $ met_standa: num  2 2 2 2 3 2 2 2 2 2 ...\n#&gt;  $ finis_book: num  2 2 2 2 2 2 2 1 2 2 ...\n#&gt;  $ long      : num  7.52 12.4 7.85 6.94 9.99 ...\n#&gt;  $ lat       : num  5.46 9.32 4.91 6.23 10.78 ...\n#&gt;  $ geometry  :sfc_MULTIPOLYGON of length 38; first list element: List of 1\n#&gt;   ..$ :List of 1\n#&gt;   .. ..$ : num [1:302, 1:2] 7.51 7.52 7.53 7.53 7.53 ...\n#&gt;   ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n#&gt;  - attr(*, \"sf_column\")= chr \"geometry\"\n#&gt;  - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n#&gt;   ..- attr(*, \"names\")= chr [1:17] \"ID_0\" \"ISO\" \"NAME_0\" \"ID_1\" ...\n\nst_is_longlat(reading) # checking whether the geographical coordinates have been \n\n#&gt; [1] TRUE\n\n#projected (the result TRUE means not) \n\nst_crs(reading) #checking which mapping was applied\n\n#&gt; Coordinate Reference System:\n#&gt;   User input: WGS 84 \n#&gt;   wkt:\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     DATUM[\"World Geodetic System 1984\",\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"latitude\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"longitude\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     ID[\"EPSG\",4326]]\n\ntable(st_is_valid(reading)) # validation\n\n#&gt; \n#&gt; TRUE \n#&gt;   38\n\nreading_sp&lt;-as(reading, \"Spatial\") \nclass(reading_sp)\n\n#&gt; [1] \"SpatialPolygonsDataFrame\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\n\n\n\nreading_points&lt;-st_cast(reading$geometry, \"MULTIPOINT\")\n\n\nreading_points_count&lt;-sapply(reading_points, length)\nsum(reading_points_count) # Checking how many vertices are in all counties\n\n#&gt; [1] 401800\n\n\n\nreading_simple&lt;-st_simplify(reading, dTolerance = 50)\n\n\nreading_simple_points&lt;-st_cast(reading_simple$geometry, \"MULTIPOINT\")\nsum(sapply(reading_simple_points, length))\n\n#&gt; [1] 73496\n\n\n\nreading_central&lt;-st_centroid(reading)\n\n\nplot(st_geometry(reading))\nreading_central&lt;-st_centroid(reading)\nplot(reading_central$geometry, add=TRUE, pch=20, col=\"red\")\n\n\n\n\n\nlibrary(ggplot2)\nggplot(reading_simple) + geom_sf() + theme_bw() \n\n\n\nggplot(reading_central) + geom_sf() + theme_bw() \n\n\n\n\n\nggplot(reading_simple) + geom_sf(aes(fill = reading_hr)) + theme_bw()\n\n\n\n\n\nggplot() + geom_sf(data=reading_simple, aes(fill=reading_hr)) +\ngeom_sf(data=reading_central, col=\"red\") + theme_bw()\n\n\n\n\n\nggplot() + geom_sf(data=reading_simple, aes(fill=books_read)) +\ngeom_sf(data=reading_central, col=\"red\") + theme_bw()\n\n\n\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf)\n\n\n\n\n\nnames(reading)\n\n#&gt;  [1] \"ID_0\"       \"ISO\"        \"NAME_0\"     \"ID_1\"       \"NAME_1\"    \n#&gt;  [6] \"TYPE_1\"     \"ENGTYPE_1\"  \"NL_NAME_1\"  \"VARNAME_1\"  \"reading_hr\"\n#&gt; [11] \"books_read\" \"cgpa\"       \"love_readi\" \"met_standa\" \"finis_book\"\n#&gt; [16] \"long\"       \"lat\"        \"geometry\"\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"reading_hr\"], \n     main = \"Spatial distn. of students' reading hour\", \n     breaks = \"quantile\")\n\n\n\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"books_read\"], \n     main = \"Spatial distn. of number of books read\", \n     breaks = \"quantile\")\n\n\n\n#legend(\"topright\", legend = \"books_read\", fill = topo.colors(5))\n\n\nreading_sf &lt;- st_read(\"reading_cultre.shp\")\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"cgpa\"], main = \"Spatial Distribution of Students' CGPA\", breaks = \"quantile\")\n\n\n\n# Add a legend\n#legend(\"topright\", legend = \"CGPA\", fill = topo.colors(5))\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"love_readi\"], \n     main = \"Spatial distn. of students who love reading\", \n     breaks = \"quantile\")\n\n\n\n#legend(\"topright\", legend = \"Love reading\", fill = topo.colors(5))\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"met_standa\"], \n     main = \"Spatial distn. of readers who read at least 1 hour per day\", \n     breaks = \"quantile\")\n\n\n\n\n\nreading_sf = st_read(\"reading_cultre.shp\") #shape file earlier created\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\nplot(reading_sf[\"finis_book\"], \n     main = \"Spatial distn. of readers who finished their last book\", \n     breaks = \"quantile\")\n\n\n\n\nSix maps in one frame\n\nlibrary(tmap)\n\n# Read shapefile\nreading_sf &lt;- st_read(\"reading_cultre.shp\")\n\n#&gt; Reading layer `reading_cultre' from data source \n#&gt;   `C:\\Users\\user\\Desktop\\softdataconsult.github.io\\blog\\posts\\spatial\\reading_cultre.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 38 features and 17 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 2.668431 ymin: 4.270418 xmax: 14.67642 ymax: 13.89201\n#&gt; Geodetic CRS:  WGS 84\n\n# Set up a 2x3 layout with tmap\ntm_layout &lt;- tm_layout(title = c(\"Reading Hour\", \"Books Read\", \"CGPA\", \"Love Reading\", \"At least 1 hr/day\", \"Finished last book\"),\n                       frame = FALSE,\n                       asp = 0)  # Set aspect ratio to 0 for individual map customization\n\n# Plot the first map\ntm1 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"reading_hr\", title = \"Reading Hour\", style = \"quantile\")\n\n# Plot the second map\ntm2 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"books_read\", title = \"Number of Books Read\", style = \"quantile\")\n\n# Plot the third map\ntm3 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"cgpa\", title = \"CGPA\", style = \"quantile\")\n\n# Plot the fourth map\ntm4 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"love_readi\", title = \"Love Reading\", style = \"quantile\")\n\n# Plot the fifth map\ntm5 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"met_standa\", title = \"At least 1 hr/day\", style = \"quantile\")\n\n# Plot the sixth map\ntm6 &lt;- tm_shape(reading_sf) +\n  tm_borders() +\n  tm_fill(\"finis_book\", title = \"Finished last book\", style = \"quantile\")\n\n# Display the maps in a 2x3 layout\ntmap_arrange(list(tm1, tm2, tm3, tm4, tm5, tm6), layout = tm_layout)\n\n\n\n\n\n# read the data from excel for other analysis\nreading_data &lt;- read.csv(\"reading_culture_coordinates.csv\")\n\n\n#define our regression equation so we don't have to type it each time\nreg.eq1=reading_hr ~ books_read + cgpa + love_readi + met_standa + finis_book\nreg1=lm(reg.eq1,data=reading)\n\nSpatial weights matrix W\n\n# Create a spatial weights matrix (queen contiguity)\nlistw &lt;- mat2listw(matrix(rbinom(nrow(reading_data)^2, 1, 0.2), nrow(reading_data)))\n\nlistw\n\n#&gt; Characteristics of weights list object:\n#&gt; Neighbour list object:\n#&gt; Number of regions: 38 \n#&gt; Number of nonzero links: 286 \n#&gt; Percentage nonzero weights: 19.80609 \n#&gt; Average number of links: 7.526316 \n#&gt; Non-symmetric neighbours list\n#&gt; \n#&gt; Weights style: M \n#&gt; Weights constants summary:\n#&gt;    n   nn  S0  S1   S2\n#&gt; M 38 1444 286 352 8872\n\n\nlm.morantest\n\nlm.morantest(reg1, listw )\n\n#&gt; \n#&gt;  Global Moran I for regression residuals\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; weights: listw\n#&gt; \n#&gt; Moran I statistic standard deviate = 2.2349, p-value = 0.01271\n#&gt; alternative hypothesis: greater\n#&gt; sample estimates:\n#&gt; Observed Moran I      Expectation         Variance \n#&gt;      0.079731616     -0.035587780      0.002662569\n\n\nLet’s run the Four simplest models: OLS, SLX, Lag Y, and Lag Error\nOLS model\n\nlibrary(spdep)\nlibrary(spatialreg)\nreg1=lm(reg.eq1,data=reading)\nreg1b=lm(reading_hr ~ books_read + cgpa + love_readi + met_standa + finis_book, data=reading)\nsummary(reg1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = reg.eq1, data = reading)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.5992 -0.5211 -0.1218  0.1665  3.3228 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept)  0.07031    0.51720   0.136   0.8927  \n#&gt; books_read   0.07339    0.08281   0.886   0.3821  \n#&gt; cgpa         0.76432    0.33255   2.298   0.0282 *\n#&gt; love_readi   0.30082    0.49804   0.604   0.5501  \n#&gt; met_standa   0.53481    0.57056   0.937   0.3556  \n#&gt; finis_book  -0.78873    0.61250  -1.288   0.2071  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9277 on 32 degrees of freedom\n#&gt; Multiple R-squared:  0.4904, Adjusted R-squared:  0.4108 \n#&gt; F-statistic: 6.159 on 5 and 32 DF,  p-value: 0.000419\n\nsummary(reg1b)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = reading_hr ~ books_read + cgpa + love_readi + met_standa + \n#&gt;     finis_book, data = reading)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.5992 -0.5211 -0.1218  0.1665  3.3228 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept)  0.07031    0.51720   0.136   0.8927  \n#&gt; books_read   0.07339    0.08281   0.886   0.3821  \n#&gt; cgpa         0.76432    0.33255   2.298   0.0282 *\n#&gt; love_readi   0.30082    0.49804   0.604   0.5501  \n#&gt; met_standa   0.53481    0.57056   0.937   0.3556  \n#&gt; finis_book  -0.78873    0.61250  -1.288   0.2071  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9277 on 32 degrees of freedom\n#&gt; Multiple R-squared:  0.4904, Adjusted R-squared:  0.4108 \n#&gt; F-statistic: 6.159 on 5 and 32 DF,  p-value: 0.000419\n\nlm.morantest(reg1,listw)\n\n#&gt; \n#&gt;  Global Moran I for regression residuals\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; weights: listw\n#&gt; \n#&gt; Moran I statistic standard deviate = 2.2349, p-value = 0.01271\n#&gt; alternative hypothesis: greater\n#&gt; sample estimates:\n#&gt; Observed Moran I      Expectation         Variance \n#&gt;      0.079731616     -0.035587780      0.002662569\n\n#lm.moranplot(reg1,listw1)\nlm.LMtests(reg1,listw,test=c(\"LMerr\", \"LMlag\", \"RLMerr\", \"RLMlag\", \"SARMA\"))\n\n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; RSerr = 1.4772, df = 1, p-value = 0.2242\n#&gt; \n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; RSlag = 1.2523, df = 1, p-value = 0.2631\n#&gt; \n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; adjRSerr = 0.711, df = 1, p-value = 0.3991\n#&gt; \n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; adjRSlag = 0.48608, df = 1, p-value = 0.4857\n#&gt; \n#&gt; \n#&gt;  Rao's score (a.k.a Lagrange multiplier) diagnostics for spatial\n#&gt;  dependence\n#&gt; \n#&gt; data:  \n#&gt; model: lm(formula = reg.eq1, data = reading)\n#&gt; test weights: listw\n#&gt; \n#&gt; SARMA = 1.9633, df = 2, p-value = 0.3747\n\n\nSLX (Spatial Lag Model with Exogenous Variables)\n\n# Assuming your data is named 'your_data'\n# Load required packages\nlibrary(spatialreg)\n\n# Create an 'sf' object with the spatial coordinates\nreading_sf_data &lt;- st_as_sf(reading_data, coords = c(\"long\", \"lat\"))\n\n# Create a spatial weights matrix (queen contiguity)\nlistw &lt;- mat2listw(matrix(rbinom(nrow(reading_data)^2, 1, 0.2), nrow(reading_data)))"
  }
]